{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c93b7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 10\n",
      "wl_key: eebff3606060fbd1c3b525bd60b7c2e26f333e9ccd19d7fe8f3115db19b8c575 found: True\n",
      "wl_key: e7117dbf8f00537f5668ed8a810301a630854c2727d6618e40d41fb67e1fe3ac found: True\n",
      "wl_key: 2966337c7a2d1ae2848dde66429c3327c40f8bdd1eda53772f1273228a6a12b5 found: True\n",
      "wl_key: d7883eaddcc3442773685867d2dadacbb8a39c6f22b98f215ac6de4dbade96fa found: True\n",
      "wl_key: 44b143a515db91f4dc19e8977b0fb522b7d48c29a1de57d35d04ca839dc3c258 found: True\n",
      "wl_key: b31cc6d1cf1b8e8cb7c1776acf0b3656246f14d9215b5491307e8e9c57c27b47 found: True\n",
      "wl_key: d3b8322a49278812dc8b315843ecd80644fb458ce78b73657a8d352f8f818351 found: True\n",
      "wl_key: d7bd7ffbf676c244a5444abd0095a512cbdfe822f164f11053762f55e0339ed2 found: True\n",
      "wl_key: 8e23030bc418651a7b55a6090abe34aff887b8ecf4dbefe5de46a97b6666b34b found: True\n",
      "wl_key: 27a954af531715d900479d28e4363dc3b09a8ad9270b3983311367bc34fb2e89 found: True\n"
     ]
    }
   ],
   "source": [
    "from phaseedge.sampling.wl_chunk_driver import fetch_wl_tip\n",
    "from phaseedge.storage.store import build_jobstore\n",
    "\n",
    "js = build_jobstore()\n",
    "\n",
    "# Query ALL CEModelDocs (payload is under \"output\" in your DB); load=True rehydrates big fields.\n",
    "rows = js.query(criteria={\"output.kind\": \"WLCheckpointDoc\"}, load=True)\n",
    "rows = list(rows)  # force evaluation\n",
    "print(\"count:\", len(rows))\n",
    "\n",
    "# Print the first ce_key (rehydrated)\n",
    "for row in rows[:10]:\n",
    "    doc = row[\"output\"]\n",
    "    tip = fetch_wl_tip(doc.get(\"wl_key\"))\n",
    "    print(\"wl_key:\", doc.get(\"wl_key\"), \"found:\" , tip is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f351c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No checkpoints found for wl_key='a378f39750be1ccc606185968d120a956ee09ab23321b5c2783f7614449ccfa4'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 240\u001b[0m\n\u001b[1;32m    233\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# ----------------------------- run ----------------------------------------\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Replace with your WL key:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# analyze_wl_chain(\"86faef5be3f2eacb173b482455f4a230366317640655494f4c4e90444a2771e7\") # MgFeO2 3x3x3 refined 50:50\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m \u001b[43manalyze_wl_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma378f39750be1ccc606185968d120a956ee09ab23321b5c2783f7614449ccfa4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# MgAl2O4 2x2x2 unrefined\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# analyze_wl_chain(\"8b19e86833ba361b9d9e833331c8fc41bbd1e2ae98a750ff95d966c70ad56ffb\") # ZnAl2O4 2x2x2 unrefined\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# analyze_wl_chain(\"cc5d9ecdceab00af670e1ed7fcc0e84a6e5eac756bc79c1c1579379bc4bffd71\") # MgGa2O4 2x2x2 unrefined\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 132\u001b[0m, in \u001b[0;36manalyze_wl_chain\u001b[0;34m(wl_key)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21manalyze_wl_chain\u001b[39m(wl_key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     wl \u001b[38;5;241m=\u001b[39m \u001b[43mload_wl_latest_from_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwl_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     levels  \u001b[38;5;241m=\u001b[39m wl[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    134\u001b[0m     entropy \u001b[38;5;241m=\u001b[39m wl[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m, in \u001b[0;36mload_wl_latest_from_chain\u001b[0;34m(wl_key)\u001b[0m\n\u001b[1;32m     27\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m fetch_wl_tip(wl_key)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m checkpoint:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo checkpoints found for wl_key=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwl_key\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m s: Mapping[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     32\u001b[0m bins: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbin_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No checkpoints found for wl_key='a378f39750be1ccc606185968d120a956ee09ab23321b5c2783f7614449ccfa4'."
     ]
    }
   ],
   "source": [
    "from typing import Any, Mapping, TypedDict, List, Tuple, Dict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from phaseedge.sampling.wl_chunk_driver import fetch_wl_tip\n",
    "from phaseedge.storage import store\n",
    "\n",
    "\n",
    "# ----------------------------- types ---------------------------------------\n",
    "\n",
    "class WLData(TypedDict):\n",
    "    levels: np.ndarray\n",
    "    entropy: np.ndarray\n",
    "    histogram: np.ndarray          # stage-local (resets at flatness)\n",
    "    occurrences: np.ndarray        # cumulative (never resets; may be absent in older ckpts)\n",
    "    bin_indices: np.ndarray\n",
    "    dE: float\n",
    "    anchor: float\n",
    "    m_log: float                   # current _m = ln(f)\n",
    "    checkpoint: Mapping[str, Any]\n",
    "\n",
    "\n",
    "# ----------------------------- loaders ------------------------------------\n",
    "\n",
    "def load_wl_latest_from_chain(wl_key: str) -> WLData:\n",
    "    checkpoint = fetch_wl_tip(wl_key)\n",
    "    if not checkpoint:\n",
    "        raise RuntimeError(f\"No checkpoints found for wl_key={wl_key!r}.\")\n",
    "    s: Mapping[str, Any] = checkpoint[\"state\"]\n",
    "\n",
    "    bins: np.ndarray = np.asarray(s[\"bin_indices\"], dtype=int)\n",
    "    entropy: np.ndarray = np.asarray(s[\"entropy\"], dtype=float)\n",
    "    hist: np.ndarray = np.asarray(s[\"histogram\"], dtype=int)\n",
    "    occs_raw = s.get(\"occurrences\", [])\n",
    "    occs: np.ndarray = (\n",
    "        np.asarray(occs_raw, dtype=int)\n",
    "        if isinstance(occs_raw, (list, tuple))\n",
    "        else np.zeros_like(hist)\n",
    "    )\n",
    "\n",
    "    anchor: float = 0.0\n",
    "    dE: float = float(s[\"bin_size\"])\n",
    "    levels: np.ndarray = anchor + dE * bins.astype(float)\n",
    "\n",
    "    order = np.argsort(bins)\n",
    "    return WLData(\n",
    "        levels=levels[order],\n",
    "        entropy=entropy[order],\n",
    "        histogram=hist[order],\n",
    "        occurrences=occs[order] if occs.size else np.zeros_like(hist[order]),\n",
    "        bin_indices=bins[order],\n",
    "        dE=dE,\n",
    "        anchor=anchor,\n",
    "        m_log=float(s[\"mod_factor\"]),  # _m = ln(f)\n",
    "        checkpoint=checkpoint,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def load_mod_schedule(wl_key: str, m0: float = 1.0) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Concatenate per-checkpoint updates into a global schedule of _m (ln f),\n",
    "    loading WL checkpoints from the Jobflow outputs store.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    steps : np.ndarray[int]\n",
    "        Monotone, starting at 0.\n",
    "    m_vals : np.ndarray[float]\n",
    "        Same length as steps; m at each change point.\n",
    "    \"\"\"\n",
    "    js = build_jobstore()\n",
    "    rows = list(\n",
    "        js.docs_store.query(\n",
    "            criteria={\"output.kind\": \"WLCheckpointDoc\", \"output.wl_key\": wl_key},\n",
    "            properties={\n",
    "                \"_id\": 0,\n",
    "                \"output.state.mod_factor\": 1,\n",
    "                \"output.mod_updates\": 1,\n",
    "                \"output.step_end\": 1,\n",
    "            },\n",
    "            sort={\"output.step_end\": 1},\n",
    "        )\n",
    "    )\n",
    "    if not rows:\n",
    "        raise RuntimeError(f\"No WL checkpoints found for wl_key={wl_key!r}.\")\n",
    "\n",
    "    # Prefer the recorded initial m from the first checkpoint, else fallback.\n",
    "    first_out = rows[0].get(\"output\", {})\n",
    "    state = first_out.get(\"state\") or {}\n",
    "    m0_eff = float(state.get(\"mod_factor\", m0))\n",
    "\n",
    "    # Collect (step, m) updates across all checkpoints\n",
    "    events: list[tuple[int, float]] = []\n",
    "    for rec in rows:\n",
    "        out = rec.get(\"output\", {})\n",
    "        for ev in out.get(\"mod_updates\", []) or []:\n",
    "            step = int(ev[\"step\"])\n",
    "            m = float(ev[\"m\"])\n",
    "            events.append((step, m))\n",
    "\n",
    "    if events:\n",
    "        # Dedup by step; keep the last m encountered for that step.\n",
    "        step_to_m: dict[int, float] = {}\n",
    "        for step, m in events:\n",
    "            step_to_m[step] = m\n",
    "        sorted_steps = sorted(step_to_m.keys())\n",
    "        steps = [0] + sorted_steps\n",
    "        m_vals = [m0_eff] + [step_to_m[s] for s in sorted_steps]\n",
    "    else:\n",
    "        tip_steps = int(rows[-1][\"output\"][\"step_end\"])\n",
    "        steps = [0, tip_steps]\n",
    "        m_vals = [m0_eff, m0_eff]\n",
    "\n",
    "    return np.asarray(steps, dtype=int), np.asarray(m_vals, dtype=float)\n",
    "\n",
    "\n",
    "def _extend_stair_to(steps: np.ndarray, vals: np.ndarray, right_x: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Ensure a step curve extends horizontally to right_x.\"\"\"\n",
    "    if steps.size == 0:\n",
    "        return np.array([0, right_x], dtype=int), np.array([1.0, 1.0], dtype=float)\n",
    "    if steps[-1] < right_x:\n",
    "        steps = np.append(steps, right_x)\n",
    "        vals  = np.append(vals,  vals[-1])\n",
    "    return steps, vals\n",
    "\n",
    "\n",
    "# ----------------------------- analysis + plotting ------------------------\n",
    "\n",
    "def analyze_wl_chain(wl_key: str) -> None:\n",
    "    wl = load_wl_latest_from_chain(wl_key)\n",
    "    levels  = wl[\"levels\"]\n",
    "    entropy = wl[\"entropy\"]\n",
    "    hist    = wl[\"histogram\"]\n",
    "    occ_all = wl[\"occurrences\"]\n",
    "    dE      = wl[\"dE\"]\n",
    "    steps_total = wl[\"checkpoint\"][\"step_end\"]\n",
    "    m_current = wl[\"m_log\"]\n",
    "    steps_m, m_vals = load_mod_schedule(wl_key)\n",
    "\n",
    "    # DOS from entropy\n",
    "    S_shift: np.ndarray = entropy - np.max(entropy) if entropy.size else entropy\n",
    "    DOS: np.ndarray = np.exp(S_shift) if S_shift.size else np.array([])\n",
    "    if DOS.size:\n",
    "        DOS /= DOS.sum()\n",
    "\n",
    "    # Cv(T)\n",
    "    kB: float = 8.617333262e-5  # eV/K\n",
    "    E:  np.ndarray = levels.astype(float)\n",
    "    E_rr: np.ndarray = E - E.min() if E.size else E\n",
    "    Ts: np.ndarray = np.linspace(100.0, 2500.0, 200)\n",
    "\n",
    "    def _weights(T: float) -> np.ndarray:\n",
    "        return np.exp(S_shift - E_rr / (kB * T))\n",
    "\n",
    "    if E.size:\n",
    "        Z  = np.array([_weights(T).sum() for T in Ts])\n",
    "        U  = np.array([(_weights(T) * E).sum() / Z[i] for i, T in enumerate(Ts)])\n",
    "        U2 = np.array([(_weights(T) * E * E).sum() / Z[i] for i, T in enumerate(Ts)])\n",
    "        Cv = (U2 - U * U) / (kB * Ts * Ts)\n",
    "    else:\n",
    "        Z = U = U2 = Cv = np.array([])\n",
    "\n",
    "    # ---- reporting ----\n",
    "    print(f\"visited bins: {levels.size} | ΔE={dE}\")\n",
    "    print(f\"total steps (chain tip): {steps_total:,}\")\n",
    "    print(f\"current _m (ln f): {m_current:.6g}  (target ≈ 1e-7)\")\n",
    "    print(f\"stage histogram sum: {int(hist.sum())} | cumulative occurrences sum: {int(occ_all.sum())}\")\n",
    "\n",
    "    # ---- sample statistics (new schema: top-level fields) ----\n",
    "    bin_samples = wl[\"checkpoint\"].get(\"bin_samples\", [])\n",
    "    samples_per_bin = int(wl[\"checkpoint\"][\"samples_per_bin\"])\n",
    "\n",
    "    if bin_samples and samples_per_bin > 0:\n",
    "        bins_to_occ: Dict[int, List[list[int]]] = defaultdict(list)\n",
    "        for rec in bin_samples:\n",
    "            try:\n",
    "                b = int(rec[\"bin\"])\n",
    "                occ = rec[\"occ\"]\n",
    "            except Exception:\n",
    "                continue\n",
    "            bins_to_occ[b].append(occ)\n",
    "\n",
    "        n_bins_with_samples = len(bins_to_occ)\n",
    "        n_bins_full = sum(1 for occs in bins_to_occ.values() if len(occs) >= samples_per_bin)\n",
    "        total_samples = sum(len(v) for v in bins_to_occ.values())\n",
    "        mean_per_sampled_bin = (total_samples / n_bins_with_samples) if n_bins_with_samples else 0.0\n",
    "\n",
    "        print(f\"sample capture: {n_bins_with_samples} bins with samples\")\n",
    "        print(f\"bins filled to quota ({samples_per_bin}): {n_bins_full}\")\n",
    "        print(f\"total stored samples: {total_samples}  (avg {mean_per_sampled_bin:.2f} per sampled bin)\")\n",
    "    else:\n",
    "        print(\"sample capture: none recorded in this checkpoint\")\n",
    "\n",
    "    # ---- plotting ----\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 9), constrained_layout=True)\n",
    "\n",
    "    ax = axs[0, 0]\n",
    "    if DOS.size:\n",
    "        eps = 0.0 if np.all(DOS > 0) else 1e-300\n",
    "        ax.semilogy(E, DOS + eps)\n",
    "    ax.set_xlabel(\"E (eV / supercell)\")\n",
    "    ax.set_ylabel(\"DOS (arb., log y)\")\n",
    "    ax.set_title(\"Density of States\")\n",
    "\n",
    "    ax = axs[0, 1]\n",
    "    if Cv.size:\n",
    "        ax.plot(Ts, Cv)\n",
    "    ax.set_xlabel(\"T (K)\")\n",
    "    ax.set_ylabel(\"C$_v$ (eV/K per supercell)\")\n",
    "    ax.set_title(\"Heat Capacity from WL\")\n",
    "\n",
    "    ax = axs[1, 0]\n",
    "    y_visits = hist if hist.sum() > 0 else occ_all\n",
    "    label = \"Histogram (stage)\" if y_visits is hist else \"Occurrences (cumulative)\"\n",
    "    if y_visits.size:\n",
    "        ax.plot(E, y_visits)\n",
    "    ax.set_xlabel(\"E (eV / supercell)\")\n",
    "    ax.set_ylabel(\"Visits\")\n",
    "    ax.set_title(f\"WL {label}\")\n",
    "\n",
    "    ax = axs[1, 1]\n",
    "    right_x = max(int(steps_total), int(steps_m[-1]) if steps_m.size else 0)\n",
    "    steps_m_ext, m_vals_ext = _extend_stair_to(steps_m, m_vals, right_x)\n",
    "    ax.step(steps_m_ext, m_vals_ext, where=\"post\")\n",
    "    ax.set_xlim(0, right_x)\n",
    "    ax.set_xlabel(\"MC steps\")\n",
    "    ax.set_ylabel(\"_m = ln(f)\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(\"WL Modification Schedule (_m)\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------- run ----------------------------------------\n",
    "\n",
    "# Replace with your WL key:\n",
    "# analyze_wl_chain(\"86faef5be3f2eacb173b482455f4a230366317640655494f4c4e90444a2771e7\") # MgFeO2 3x3x3 refined 50:50\n",
    "analyze_wl_chain(\"8f505feb913665b3f3662d4f2a1a71eb3d20517a8e7fabdc294ec3377e354983\") # MgAl2O4 2x2x2 unrefined\n",
    "# analyze_wl_chain(\"8b19e86833ba361b9d9e833331c8fc41bbd1e2ae98a750ff95d966c70ad56ffb\") # ZnAl2O4 2x2x2 unrefined\n",
    "# analyze_wl_chain(\"cc5d9ecdceab00af670e1ed7fcc0e84a6e5eac756bc79c1c1579379bc4bffd71\") # MgGa2O4 2x2x2 unrefined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00442b42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No checkpoints found for wl_key='a378f39750be1ccc606185968d120a956ee09ab23321b5c2783f7614449ccfa4' in collection 'wang_landau_ckpt'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 317\u001b[0m\n\u001b[1;32m    313\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# Example:\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_wl_chain_tip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma378f39750be1ccc606185968d120a956ee09ab23321b5c2783f7614449ccfa4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43msublattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43melement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mT_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m, in \u001b[0;36manalyze_wl_chain_tip\u001b[0;34m(wl_key, sublattice, element, T_grid, collection_name)\u001b[0m\n\u001b[1;32m     55\u001b[0m doc \u001b[38;5;241m=\u001b[39m coll\u001b[38;5;241m.\u001b[39mfind_one({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwl_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: wl_key}, sort\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo checkpoints found for wl_key=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwl_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in collection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m doc_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(doc\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     60\u001b[0m production_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(doc\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduction_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mValueError\u001b[0m: No checkpoints found for wl_key='a378f39750be1ccc606185968d120a956ee09ab23321b5c2783f7614449ccfa4' in collection 'wang_landau_ckpt'."
     ]
    }
   ],
   "source": [
    "from typing import Any, Mapping\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from phaseedge.storage import store  # uses your env-configured connections\n",
    "\n",
    "\n",
    "__all__ = [\"analyze_wl_chain_tip\"]\n",
    "\n",
    "\n",
    "def analyze_wl_chain_tip(\n",
    "    wl_key: str,\n",
    "    *,\n",
    "    sublattice: str,\n",
    "    element: str,\n",
    "    T_grid: list[float],\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Read tip-of-chain cation composition stats and WL state for a WL key (RO connection),\n",
    "    validate production mode, compute x(E) and x(T) for a single (sublattice, element),\n",
    "    and plot a single two-subplot figure: left = x(E), right = x(T).\n",
    "\n",
    "    Args:\n",
    "        wl_key: Wang-Landau chain key to query.\n",
    "        sublattice: Sublattice label to plot (must match what's stored in cation_counts).\n",
    "        element: String element code to plot.\n",
    "        T_grid: Temperatures (K) for x(T). Defaults to 200-2000 K, 200 points.\n",
    "        species_map: Optional {element_code -> pretty label} for plot titles.\n",
    "\n",
    "    Returns:\n",
    "        Dict bundle with raw/derived data (same structure as before, plus 'series'):\n",
    "          {\n",
    "            \"doc_id\": str,\n",
    "            \"bin_size\": float,\n",
    "            \"bin_indices\": np.ndarray[int],\n",
    "            \"entropy\": np.ndarray[float],\n",
    "            \"occurrences_map\": dict[int, int],\n",
    "            \"E_map\": dict[int, float],\n",
    "            \"S_map\": dict[int, float],\n",
    "            \"cation_agg\": dict[int, dict[str, dict[int, dict[int, int]]]],\n",
    "            \"xE\": dict[str, dict[int, dict[int, float]]],\n",
    "            \"xT\": dict[str, dict[int, dict[float, float]]],\n",
    "            \"T_grid\": list[float],\n",
    "            \"series\": {\"sublattice\": str, \"element\": str},\n",
    "          }\n",
    "    \"\"\"\n",
    "    # -------------------------\n",
    "    # 1) Connect & fetch TIP\n",
    "    # -------------------------\n",
    "    doc = fetch_wl_tip(wl_key)\n",
    "    if doc is None:\n",
    "        raise ValueError(f\"No checkpoints found for wl_key='{wl_key}'.\")\n",
    "    doc_id = str(doc.get(\"_id\", \"\"))\n",
    "    production_mode = bool(doc.get(\"production_mode\", False))\n",
    "    if not production_mode:\n",
    "        raise ValueError(\n",
    "            f\"Checkpoint {doc_id} for wl_key='{wl_key}' is not in production_mode. \"\n",
    "            \"Choose a production-mode tip or resume the chain in production mode.\"\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Extract WL state\n",
    "    # -------------------------\n",
    "    state = _get_state(doc)\n",
    "    bin_indices = np.asarray(state.get(\"bin_indices\", []), dtype=int)\n",
    "    entropy = np.asarray(state.get(\"entropy\", []), dtype=float)\n",
    "    occurrences = np.asarray(state.get(\"occurrences\", []), dtype=int)\n",
    "    bin_size = float(state.get(\"bin_size\", np.nan))\n",
    "\n",
    "    if not np.isfinite(bin_size) or bin_indices.size == 0 or entropy.size == 0:\n",
    "        raise ValueError(\n",
    "            f\"Missing/incomplete WL state in checkpoint {doc_id}. \"\n",
    "            \"Require bin_indices (visited), entropy, and bin_size.\"\n",
    "        )\n",
    "\n",
    "    if occurrences.size != bin_indices.size:\n",
    "        occ_map: dict[int, int] = {int(b): 0 for b in bin_indices.tolist()}\n",
    "        n = int(min(occurrences.size, bin_indices.size))\n",
    "        for i in range(n):\n",
    "            occ_map[int(bin_indices[i])] = int(occurrences[i])\n",
    "        occurrences = np.asarray([occ_map[int(b)] for b in bin_indices], dtype=int)\n",
    "\n",
    "    E_map: dict[int, float] = {int(b): float(b) * bin_size for b in bin_indices.tolist()}\n",
    "    S_map: dict[int, float] = {int(b): float(entropy[i]) for i, b in enumerate(bin_indices.tolist())}\n",
    "    occurrences_map: dict[int, int] = {int(b): int(occurrences[i]) for i, b in enumerate(bin_indices.tolist())}\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Extract cation_counts\n",
    "    # -------------------------\n",
    "    rows = _get_cation_rows(doc)\n",
    "    cation_agg = _aggregate_cation_counts(rows)\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Compute x(E)\n",
    "    # -------------------------\n",
    "    xE = _compute_xE(cation_agg)\n",
    "\n",
    "    # -------------------------\n",
    "    # 5) Compute x(T)\n",
    "    # -------------------------\n",
    "    xT = _compute_xT(xE, S_map, E_map, T_grid)\n",
    "\n",
    "    # -------------------------\n",
    "    # 6) Single two-subplot figure for (sublattice, element)\n",
    "    # -------------------------\n",
    "    _plot_single_xE_xT(\n",
    "        sublattice=sublattice,\n",
    "        element=element,\n",
    "        xE=xE,\n",
    "        xT=xT,\n",
    "        E_map=E_map,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"bin_size\": bin_size,\n",
    "        \"bin_indices\": bin_indices,\n",
    "        \"entropy\": entropy,\n",
    "        \"occurrences_map\": occurrences_map,\n",
    "        \"E_map\": E_map,\n",
    "        \"S_map\": S_map,\n",
    "        \"cation_agg\": cation_agg,\n",
    "        \"xE\": xE,\n",
    "        \"xT\": xT,\n",
    "        \"T_grid\": list(T_grid),\n",
    "        \"series\": {\"sublattice\": sublattice, \"element\": element},\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers (typed & local)\n",
    "# =========================\n",
    "\n",
    "def _get_state(doc: Mapping[str, Any]) -> Mapping[str, Any]:\n",
    "    \"\"\"Support 'state' (preferred) and 'kernel_state' (fallback).\"\"\"\n",
    "    state = doc.get(\"state\")\n",
    "    if isinstance(state, dict):\n",
    "        return state\n",
    "    ks = doc.get(\"kernel_state\")\n",
    "    if isinstance(ks, dict):\n",
    "        return ks\n",
    "    return {}\n",
    "\n",
    "\n",
    "def _get_cation_rows(doc: Mapping[str, Any]) -> list[Mapping[str, Any]]:\n",
    "    \"\"\"Pull flattened cation_counts list from the top-level doc (schema_version >= 3).\"\"\"\n",
    "    rows = doc.get(\"cation_counts\")\n",
    "    if isinstance(rows, list):\n",
    "        return [r for r in rows if isinstance(r, dict)]\n",
    "    stats = doc.get(\"stats\")\n",
    "    if isinstance(stats, dict):\n",
    "        rows2 = stats.get(\"cation_counts\")\n",
    "        if isinstance(rows2, list):\n",
    "            return [r for r in rows2 if isinstance(r, dict)]\n",
    "    return []\n",
    "\n",
    "\n",
    "def _aggregate_cation_counts(\n",
    "    rows: list[Mapping[str, Any]]\n",
    ") -> dict[int, dict[str, dict[str, dict[int, int]]]]:\n",
    "    \"\"\"\n",
    "    Aggregate cation rows into:\n",
    "      cation_agg[bin][sublattice][element_code][n_sites] = visits\n",
    "    \"\"\"\n",
    "    agg: dict[int, dict[str, dict[str, dict[int, int]]]] = {}\n",
    "    for r in rows:\n",
    "        try:\n",
    "            b = int(r[\"bin\"])\n",
    "            sl = str(r[\"sublattice\"])\n",
    "            elem = r[\"element\"]\n",
    "            ns = int(r[\"n_sites\"])\n",
    "            cnt = int(r[\"count\"])\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Malformed cation_counts row: {r}\") from e\n",
    "        agg.setdefault(b, {}).setdefault(sl, {}).setdefault(elem, {})\n",
    "        agg[b][sl][elem][ns] = agg[b][sl][elem].get(ns, 0) + cnt\n",
    "    return agg\n",
    "\n",
    "\n",
    "def _compute_xE(\n",
    "    cation_agg: dict[int, dict[str, dict[str, dict[int, int]]]]\n",
    ") -> dict[str, dict[str, dict[int, float]]]:\n",
    "    \"\"\"\n",
    "    xE[sl][elem][bin] = fraction of sublattice occupied by element in that bin.\n",
    "\n",
    "    For a fixed (bin, sl):\n",
    "        Denominator D = sum_elem sum_n (n_sites * visits)\n",
    "        Numerator for elem = sum_n (n_sites * visits)\n",
    "        x_E = Numerator / D\n",
    "    \"\"\"\n",
    "    xE: dict[str, dict[str, dict[int, float]]] = {}\n",
    "    for b, sl_dict in cation_agg.items():\n",
    "        for sl, elem_dict in sl_dict.items():\n",
    "            den = 0.0\n",
    "            for hist in elem_dict.values():\n",
    "                for n_sites, cnt in hist.items():\n",
    "                    den += float(n_sites) * float(cnt)\n",
    "            if den <= 0.0:\n",
    "                continue\n",
    "            for elem, hist in elem_dict.items():\n",
    "                num = 0.0\n",
    "                for n_sites, cnt in hist.items():\n",
    "                    num += float(n_sites) * float(cnt)\n",
    "                frac = num / den\n",
    "                xE.setdefault(sl, {}).setdefault(elem, {})[b] = float(frac)\n",
    "    return xE\n",
    "\n",
    "\n",
    "def _compute_xT(\n",
    "    xE: dict[str, dict[str, dict[int, float]]],\n",
    "    S_map: dict[int, float],\n",
    "    E_map: dict[int, float],\n",
    "    T_grid: list[float],\n",
    ") -> dict[str, dict[str, dict[float, float]]]:\n",
    "    \"\"\"\n",
    "    Canonical mixing:\n",
    "        p_b(T) ∝ exp(S(b) - E(b)/(k_B T))\n",
    "        x_T(sl,elem; T) = sum_b p_b(T) * x_E(sl,elem; b)\n",
    "\n",
    "    Uses a numerically stable softmax in log-space to avoid overflow.\n",
    "    \"\"\"\n",
    "    kB_eV_per_K = 8.617333262145e-5\n",
    "    T_arr = np.asarray(T_grid, dtype=float)\n",
    "    xT: dict[str, dict[str, dict[float, float]]] = {}\n",
    "\n",
    "    for sl, elem_bins in xE.items():\n",
    "        for elem, xE_bins in elem_bins.items():\n",
    "            bins = [int(b) for b in xE_bins.keys() if int(b) in S_map and int(b) in E_map]\n",
    "            if len(bins) == 0:\n",
    "                continue\n",
    "            bins = sorted(bins)\n",
    "            S_vec = np.asarray([S_map[int(b)] for b in bins], dtype=float)\n",
    "            E_vec = np.asarray([E_map[int(b)] for b in bins], dtype=float)\n",
    "            xE_vec = np.asarray([xE_bins[int(b)] for b in bins], dtype=float)\n",
    "\n",
    "            inv_kT = 1.0 / (kB_eV_per_K * T_arr.reshape(-1, 1))  # shape (T, 1)\n",
    "            logw = S_vec.reshape(1, -1) - E_vec.reshape(1, -1) * inv_kT\n",
    "\n",
    "            for ti, T in enumerate(T_arr.tolist()):\n",
    "                lw = logw[ti, :]\n",
    "                weights = _softmax_from_logs(lw)\n",
    "                xT_val = float(np.dot(weights, xE_vec))\n",
    "                xT.setdefault(sl, {}).setdefault(elem, {})[float(T)] = xT_val\n",
    "    return xT\n",
    "\n",
    "\n",
    "def _softmax_from_logs(log_weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Stable softmax for 1D log-weights.\"\"\"\n",
    "    if log_weights.size == 0:\n",
    "        return np.asarray([], dtype=float)\n",
    "    m = float(np.max(log_weights))\n",
    "    w = np.exp(log_weights - m)\n",
    "    s = float(np.sum(w))\n",
    "    if s == 0.0 or not math.isfinite(s):\n",
    "        return np.ones_like(log_weights) / float(log_weights.size)\n",
    "    return w / s\n",
    "\n",
    "\n",
    "def _plot_single_xE_xT(\n",
    "    *,\n",
    "    sublattice: str,\n",
    "    element: str,\n",
    "    xE: dict[str, dict[str, dict[int, float]]],\n",
    "    xT: dict[str, dict[str, dict[float, float]]],\n",
    "    E_map: dict[int, float],\n",
    ") -> None:\n",
    "    \"\"\"Render one figure with two subplots for the requested (sublattice, element).\"\"\"\n",
    "    xE_bins = xE.get(sublattice, {}).get(element, {})\n",
    "    xT_series = xT.get(sublattice, {}).get(element, {})\n",
    "\n",
    "    if len(xE_bins) == 0 and len(xT_series) == 0:\n",
    "        raise ValueError(\n",
    "            f\"No x(E) or x(T) data for sublattice='{sublattice}', element={element}. \"\n",
    "            \"Confirm the labels/codes exist in the checkpoint.\"\n",
    "        )\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.suptitle(f\"{element} on sublattice '{sublattice}'\")\n",
    "\n",
    "    # Left: x(E)\n",
    "    ax = axes[0]\n",
    "    if len(xE_bins) > 0:\n",
    "        bins_sorted = sorted(xE_bins.keys())\n",
    "        E_series = np.asarray([E_map[int(b)] for b in bins_sorted], dtype=float)\n",
    "        y_series = np.asarray([xE_bins[int(b)] for b in bins_sorted], dtype=float)\n",
    "        ax.plot(E_series, y_series, marker=\"o\")\n",
    "    ax.set_xlabel(\"E (eV per supercell)\")\n",
    "    ax.set_ylabel(\"x (fraction)\")\n",
    "    ax.set_title(\"x(E)\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Right: x(T)\n",
    "    ax = axes[1]\n",
    "    if len(xT_series) > 0:\n",
    "        Ts = sorted(xT_series.keys())\n",
    "        ys = [xT_series[T] for T in Ts]\n",
    "        ax.plot(np.asarray(Ts, dtype=float), np.asarray(ys, dtype=float), marker=\"o\")\n",
    "    else:\n",
    "        # If no xT for this pair, plot empty axes so the layout stays stable.\n",
    "        ax.plot([], [])\n",
    "    ax.set_xlabel(\"T (K)\")\n",
    "    ax.set_ylabel(\"x (fraction)\")\n",
    "    ax.set_title(\"x(T)\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example:\n",
    "result = analyze_wl_chain_tip(\n",
    "    \"a378f39750be1ccc606185968d120a956ee09ab23321b5c2783f7614449ccfa4\",\n",
    "    sublattice=\"Es\",\n",
    "    element=\"Al\",\n",
    "    T_grid=np.linspace(200.0, 2000.0, 200).astype(float).tolist(),\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atomate2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
